---
layout: post
title:  "搭建hadoop伪分布集群"
date:   2017-08-26 11:38:00 +0800
categories: Hadoop
tags: 安装教程
author: Jeff
mathjax: true
---

* content
{:toc}


# 基础配置
1. 关闭防火墙和selinux    
chkconfig iptables off    
vim /etc/selinux/config    
![](http://ov7z79pcc.bkt.clouddn.com/15037291026190.jpg)

2. 修改hostname，并将域名和ip的映射关系写入hosts文件    
vim /etc/sysconfig/network    
![](http://ov7z79pcc.bkt.clouddn.com/15037291270890.jpg)    
vim /etc/hosts    
![](http://ov7z79pcc.bkt.clouddn.com/15037291439054.jpg)

3. 重启并查看相关配置是否生效    
![](http://ov7z79pcc.bkt.clouddn.com/15037291621267.jpg)

4. 同步系统时间    
ntpdate ntp1.aliyun.com    
![](http://ov7z79pcc.bkt.clouddn.com/15037291814068.jpg)

5. 创建hadoop用户，并修改其登录密码    
useradd -m Hadoop    
passwd hadoop    
![](http://ov7z79pcc.bkt.clouddn.com/15037292020082.jpg)

6. 配置免密码登录，su到hadoop用户    
![](http://ov7z79pcc.bkt.clouddn.com/15037292207645.jpg)

7. 生成密钥文件，并将公钥写入到认证文件中    
ssh-keygen –t rsa    
![](http://ov7z79pcc.bkt.clouddn.com/15037292417953.jpg)

8. 为.ssh目录授权    
![](http://ov7z79pcc.bkt.clouddn.com/15037292610825.jpg)

9. ssh一下自己的服务器,验证免密码登录    
![](http://ov7z79pcc.bkt.clouddn.com/15037292778022.jpg)

# 安装jdk    
1. 创建放置软件的目录，并将软件上传    
mkdir app    
cd app/    
进入到app目录将软件包上传

2. 解压软件包，并创建软连接    
tar zxvf java.tar.gz     
ln -s jdk1.7.0_80 jdk

3. 在用户的配置文件中,配置jdk环境变量    
vim ~/.bashrc     
![](http://ov7z79pcc.bkt.clouddn.com/15037293678779.jpg)    
重新读取配置文件使其生效    
. ~/.bashrc

4. 查看是否安装成功    
![](http://ov7z79pcc.bkt.clouddn.com/15037294033187.jpg)

# 建hadoop伪分布集群    
1. 进入到hadoop的配置文件目录(hadoop里的etc文件夹)并修改core-site.xml配置文件    

```
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://h1:9000</value>
        <!-- 设置HDFS服务的主机名和端口号-->
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/home/hadoop/data/tmp</value>
        <!--这里的路径默认是 NameNode 、 DataNode 等存放数据的公共临时目录-->
    </property>
    <property>
        <name>hadoop.proxyuser.hadoop.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hadoop.groups</name>
        <value>*</value>
    </property>
</configuration>

```

2. 修改hdfs-site.xml配置文件
```
<configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/home/hadoop/data/dfs/name</value>
        <final>true</final>
        <!--设置hdfs中的Namenode文件目录-->
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/home/hadoop/data/dfs/data</value>
        <final>true</final>
        <!--设置hdfs中的datanode文件目录-->
    </property>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
        <!--设置数据块副本-->
    </property>
    <property>
        <name>dfs.permissions</name>
        <value>false</value>
        <!--hdfs的访问权限设置为false-->
    </property>
</configuration>
```

3. 修改hadoop-env.sh配置文件，增加一条配置
![](http://ov7z79pcc.bkt.clouddn.com/15037294691309.jpg)

4. 修改mapred-site.xml配置文件
```
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
        <!--指定运行 mapreduce 的环境为yarn-->
    </property>
</configuration>
```

5. 修改yarn-site.xml配置文件
```
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
        <!--为了能够运行MapReduce程序，需要让NodeManager在启动时加载shuffle server-->
    </property>
</configuration>
```

6. 修改slaves配置文件     
![](http://ov7z79pcc.bkt.clouddn.com/15037295149322.jpg)

7. 创建hadoop目录的软连接    
![](http://ov7z79pcc.bkt.clouddn.com/15037295333260.jpg)

8. 配置hadoop环境变量，新增红框配置    
vim ~/.bashrc    
![](http://ov7z79pcc.bkt.clouddn.com/15037295493232.jpg)    
重新读取配置文件使其生效    
. ~/.bashrc

9. 创建hadoop相关数据目录，确保创建位置在/home/hadoop下    
cd    
mkdir -p data/tmp    
mkdir -p data/dfs/name    
mkdir -p data/dfs/data

10. 格式化Namenode    
![](http://ov7z79pcc.bkt.clouddn.com/15037295928044.jpg)

11. 启动伪分布集群    
![](http://ov7z79pcc.bkt.clouddn.com/15037296063910.jpg)

12. Web UI查看(已在本机host文件中写入了域名和ip的映射关系)    
![](http://ov7z79pcc.bkt.clouddn.com/15037296210335.jpg)    
![](http://ov7z79pcc.bkt.clouddn.com/15037296261931.jpg)

13. 创建目录并上传文件进行测试    
bin/hdfs dfs -mkdir /dajiangtai    
bin/hdfs dfs -put abc.txt /dajiangtai    
![](http://ov7z79pcc.bkt.clouddn.com/15037296567330.jpg)    
bin/Hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /dajiangtai/abc.txt /dajiangtai/ret-output    
![](http://ov7z79pcc.bkt.clouddn.com/15037296810725.jpg)

14. 得到结果    
![](http://ov7z79pcc.bkt.clouddn.com/15037297000486.jpg)



