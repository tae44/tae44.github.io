---
layout: post
title:  "Sqoop的安装及基本操作说明"
date:   2017-11-23 13:40:18 +0800
categories: Hadoop
tags: sqoop
author: Jeff
mathjax: true
---

* content
{:toc}


# Sqoop安装
* 1 下载sqoop安装包并解压(在之前hive的主机上操作)    
    cd app    
    wget http://apache.osuosl.org/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz    
    tar zxf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz    
    rm sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz    
    mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop

* 2 配置环境变量    
    sudo vim /etc/profile
    ![](http://ov7z79pcc.bkt.clouddn.com/15114141450806.jpg)

* 3 将mysql的jar包放到sqoop的lib目录下    
    cd /home/hadoop/app/sqoop/lib    
    mv /home/hadoop/app/hive/lib/mysql-connector-java-5.1.44-bin.jar .

# 基本命令的使用
* 1 明文密码登录查询    
    sqoop list-databases --connect jdbc:mysql://192.168.126.189 --username root --password 123456
    ![](http://ov7z79pcc.bkt.clouddn.com/15114153221534.jpg)

* 2 交互密码登录查询    
    sqoop list-databases --connect jdbc:mysql://192.168.126.189 --username root -P
    ![](http://ov7z79pcc.bkt.clouddn.com/15114154602530.jpg)

* 3 使用hdfs上的密码文件登录查询
    * 创建密码文件    
        echo -n '123456' > .password
        
    * 将其上传至hdfs中    
        hadoop fs -mkdir /data    
        hadoop fs -put .password /data    
        hadoop fs -ls /data
        ![](http://ov7z79pcc.bkt.clouddn.com/15114156624463.jpg)

    * 查看列表    
        sqoop list-databases --connect jdbc:mysql://192.168.126.189 --username root --password-file /data/.password

* 4 不指定位置将mysql中的表格导入hdfs中<br>
    sqoop import --connect jdbc:mysql://192.168.126.189/gjp --username root --password-file /data/.password --table users<br>
    hadoop fs -ls /user/hadoop<br>
    hadoop fs -ls /user/hadoop/users
    ![](http://ov7z79pcc.bkt.clouddn.com/15114163765607.jpg)

* 5 指定位置将mysql中的表格导入hdfs中<br>
    sqoop import --connect jdbc:mysql://192.168.126.189/gjp --username root --password-file /data/.password --table users --target-dir /sqoop/test/djt_user<br>
    hadoop fs -ls /sqoop/test
    ![](http://ov7z79pcc.bkt.clouddn.com/15114166588478.jpg)

* 6 将5的命令重新执行,发现错误,原因是输出目录已存在,需要加参数才可覆盖导入操作<br>
    sqoop import --connect jdbc:mysql://192.168.126.189/gjp --username root --password-file /data/.password --table users --target-dir /sqoop/test/djt_user --delete-target-dir
    ![](http://ov7z79pcc.bkt.clouddn.com/15114167951558.jpg)

* 7 指定Map任务数导入<br>
    sqoop import --connect jdbc:mysql://192.168.126.189/gjp --username root --password-file /data/.password --table users --target-dir /sqoop/test/djt_user --delete-target-dir -m 1
    ![](http://ov7z79pcc.bkt.clouddn.com/15114169630629.jpg)
    ![](http://ov7z79pcc.bkt.clouddn.com/15114170699317.jpg)

* 8 指定数据分隔符导入,跟7的图片进行比较<br>
    sqoop import --connect jdbc:mysql://192.168.126.189/gjp --username root --password-file /data/.password --table users --target-dir /sqoop/test/djt_user --delete-target-dir -m 1 --fields-terminated-by "|"
    ![](http://ov7z79pcc.bkt.clouddn.com/15114172326468.jpg)

* 9 增量更新导入数据,之前表里有3行数据<br>
    sqoop import --connect jdbc:mysql://192.168.126.189/gjp --username root --password-file /data/.password --table users --target-dir /sqoop/test/djt_user -m 1 --fields-terminated-by "|" --append --check-column 'id' --incremental append --last-value 3
    ![](http://ov7z79pcc.bkt.clouddn.com/15114176013471.jpg)

* 10 利用job进行增量导入更新,现在表里有5行数据<br>
    sqoop job --create import_user -- import --connect jdbc:mysql://192.168.126.189/gjp --username root --password-file /data/.password --table users --target-dir /sqoop/test/djt_user -m 1 --fields-terminated-by "|" --append --check-column 'id' --incremental append --last-value 5<br>
    sqoop job --list
    ![](http://ov7z79pcc.bkt.clouddn.com/15114178197800.jpg)<br>
    sqoop job --exec import_user
    ![](http://ov7z79pcc.bkt.clouddn.com/15114179615116.jpg)
    如果之后数据库增加了数据,继续执行此命令即可<br>
    sqoop job --exec import_user
    ![](http://ov7z79pcc.bkt.clouddn.com/15114180853217.jpg)

* 11 压缩导入<br>
    sqoop import --connect jdbc:mysql://192.168.126.189/gjp --username root --password-file /data/.password --table users --target-dir /sqoop/test/djt_user --delete-target-dir -m 1 --fields-terminated-by "|" -z
    ![](http://ov7z79pcc.bkt.clouddn.com/15114182102392.jpg)

* 12 空值替换导入<br>
    sqoop import --connect jdbc:mysql://192.168.126.189/gjp --username root --password-file /data/.password --table users --target-dir /sqoop/test/djt_user --delete-target-dir -m 1 --fields-terminated-by "|" --null-non-string "###" --null-string "###"
    ![](http://ov7z79pcc.bkt.clouddn.com/15114185337879.jpg)
    ![](http://ov7z79pcc.bkt.clouddn.com/15114185675791.jpg)

* 13 指定某列导入<br>
    sqoop import --connect jdbc:mysql://192.168.126.189/gjp --username root --password-file /data/.password --table users --columns name,age --target-dir /sqoop/test/djt_user --delete-target-dir -m 1 --fields-terminated-by "|"
    ![](http://ov7z79pcc.bkt.clouddn.com/15114186886727.jpg)

* 14 带where条件的查询导入<br>
    sqoop import --connect jdbc:mysql://192.168.126.189/gjp --username root --password-file /data/.password --table users --where "age>30" --target-dir /sqoop/test/djt_user --delete-target-dir -m 1 --fields-terminated-by "|"<br>
    ![](http://ov7z79pcc.bkt.clouddn.com/15114187856830.jpg)

* 15 使用query的模式查询导入(查询语句使用单引号括起来\$CONDITIONS无需转义)<br>
    sqoop import --connect jdbc:mysql://192.168.126.189/gjp --username root --password-file /data/.password --query 'SELECT * FROM users WHERE age<=18 AND $CONDITIONS' --target-dir /sqoop/test/djt_user --delete-target-dir -m 1 --fields-terminated-by "|"
    ![](http://ov7z79pcc.bkt.clouddn.com/15114189006318.jpg)

* 16 使用query的模式查询导入(查询语句使用双引号括起来$CONDITIONS需转义)<br>
    sqoop import --connect jdbc:mysql://192.168.126.189/gjp --username root --password-file /data/.password --query "SELECT * FROM users WHERE age<=25 AND \$CONDITIONS" --target-dir /sqoop/test/djt_user --delete-target-dir -m 1 --fields-terminated-by "|"
    ![](http://ov7z79pcc.bkt.clouddn.com/15114190073398.jpg)

* 17 将hdfs上的数据导出到mysql中,复制的表需要提前在mysql中创建<br>
    sqoop export --connect jdbc:mysql://192.168.126.189/gjp --username root --password-file /data/.password --table users_copy --export-dir /sqoop/test/djt_user -m 1 --fields-terminated-by "|"
    ![](http://ov7z79pcc.bkt.clouddn.com/15114196547153.jpg)

* 18 增量导出<br>
    sqoop export --connect jdbc:mysql://192.168.126.189/gjp --username root --password-file /data/.password --table users_copy --export-dir /sqoop/test/djt_user -m 1 --fields-terminated-by "|" --update-key id --update-mode allowinsert

* 19 事务导出,中间表需要提前在mysql中创建<br>
    sqoop export --connect jdbc:mysql://192.168.126.189/gjp --username root --password-file /data/.password --table users_copy --export-dir /sqoop/test/djt_user -m 1 --fields-terminated-by "|" --staging-table users_copy_tmp --clear-staging-table
    
* 20 指定要导出的列<br>
    sqoop export --connect jdbc:mysql://192.168.126.189/gjp --username root --password-file /data/.password --table users_copy --columns username,age --export-dir /sqoop/test/djt_user -m 1 --fields-terminated-by "|"
    ![](http://ov7z79pcc.bkt.clouddn.com/15114202421162.jpg)

# 将sqoop命令编写为脚本
```shell
#! /bin/sh
source /etc/profile
source ~/app/script/param/sq_shell.config
source ~/.bashrc
#表名称
v_tableName=hive
#需要导入的表字段
v_fields=stockid,tradedate,maxprice,minprice
#HDFS数据存放地址
v_src=/data/000000_0
sqoop export \
--connect $CONNECTURL \
--username $USERNAME \
--password $PASSWORD \
--table $v_tableName \
--export-dir $v_src \
--columns $v_fields \
-m 1
```

