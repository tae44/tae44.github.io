---
layout: post
title:  "编译安装spark"
date:   2017-12-31 20:30:55 +0800
categories: Hadoop
tags: Spark
author: Jeff
mathjax: true
---

* content
{:toc}


# 环境介绍
* 1 系统及使用软件    
    Centos6.8-64位系统    
    hadoop-2.6.0    
    Java-1.8.0_151     
    spark-2.1.2    
    maven-3.5.2

* 2 实验注意事项<br>
    * 总体Hadoop实验环境请参考我之前的博客    
    * 如未说明则一律在h1主机上使用hadoop用户操作

# 编译安装spark
* 1 安装依赖的软件包,可能你的环境跟我不一样,如果编译过程中出现其他问题,请自行百度<br>
    yum -y install epel*<br>
    yum -y install gcc-c++ cmake zlib openssl-devel R git

* 2 安装maven,配置环境变量<br>
    cd /home/hadoop/app<br>
    wget http://apache.claz.org/maven/maven-3/3.5.2/binaries/apache-maven-3.5.2-bin.tar.gz<br>
    tar zxf apache-maven-3.5.2-bin.tar.gz<br>
    rm apache-maven-3.5.2-bin.tar.gz<br>
    mv apache-maven-3.5.2 maven

* 3 下载spark源码包<br>
    cd /home/hadoop/app<br>
    wget http://mirrors.ibiblio.org/apache/spark/spark-2.1.2/spark-2.1.2.tgz<br>
    tar zxf spark-2.1.2.tgz<br>
    rm spark-2.1.2.tgz<br>
    mv spark-2.1.2 spark

* 4 更换mvn,并通过mvn下载依赖的各种的jar包<br>
    * 更换mvn为自己下载的版本<br>
        cd spark<br>
        vim dev/make-distribution.sh
        ```
        MVN="/home/hadoop/app/maven/bin/mvn"
        ```
        ![](http://ov7z79pcc.bkt.clouddn.com/15147238388936.jpg)
    
    * 调节mvn使用的内存大小<br>
        export MAVEN_OPTS="-Xmx2g -XX:ReservedCodeCacheSize=512m"
        
    * 下载依赖,这里时间会比较长,而且会出各种各样的问题,建议多试几次<br>
        mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -Psparkr -DskipTests clean package<br>
        ![](http://ov7z79pcc.bkt.clouddn.com/15147239026931.jpg)

* 5 编译spark,hadoop版本换成你自己需要的<br>
    ./dev/make-distribution.sh --name my-spark --tgz -Psparkr -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -Pyarn<br>
    ![](http://ov7z79pcc.bkt.clouddn.com/15147239663496.jpg)

* 6 安装编译好的spark并测试使用<br>
    cp spark-2.1.2-bin-my-spark.tgz /home/hadoop/app<br>
    cd /home/hadoop/app<br>
    tar zxf spark-2.1.2-bin-my-spark.tgz<br>
    rm spark-2.1.2-bin-my-spark.tgz<br>
    cd spark-2.1.2-bin-my-spark/bin<br>
    ./run-example SparkPi
    ![](http://ov7z79pcc.bkt.clouddn.com/15147240661777.jpg)

* 7 跑一个简单程序<br>
    * 准备测试数据<br>
        cd /home/hadoop/data<br>
        vim spark.txt<br>
        ![](http://ov7z79pcc.bkt.clouddn.com/15147241296313.jpg)

    * 进入spark的shell环境<br>
        cd /home/hadoop/app/spark-2.1.2-bin-my-spark/bin<br>
        ./spark-shell<br>
        ![](http://ov7z79pcc.bkt.clouddn.com/15147242096531.jpg)

    * 运行spark命令<br>
        ```val line = sc.textFile("/home/hadoop/data/spark.txt")
           line.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_).collect().foreach(println)
        ```
    ![](http://ov7z79pcc.bkt.clouddn.com/15147242389232.jpg)


