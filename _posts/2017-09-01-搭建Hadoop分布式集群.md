---
layout: post
title:  "搭建Hadoop分布式集群"
date:   2017-09-01 21:15:20 +0800
categories: Hadoop
tags: 分布式集群
author: Jeff
mathjax: true
---

* content
{:toc}


# 环境介绍
* 1 系统及使用软件    
    Centos6.8-64位系统    
    hadoop-2.6.0    
    Java-1.7.80    
    zookeeper-3.4.9

* 2 实验拓扑及实验注意事项<br>
    h1,h2节点角色: namenode,resourcemanager,journalnode,zookeeper    
    h3,h4,h5节点角色: datanode,journalnode,zookeeper    
    **PS: Journalnode和ZooKeeper保持奇数个，最少不少于3个节点。实验中使用的安装包，如未给出下载地址，请自行去官网下载。当创建好hadoop用户后，如未说明则操作一律使用hadoop用户进行。** 
    ![](http://ov7z79pcc.bkt.clouddn.com/15043115750991.jpg)

# 基础配置（在5台主机上分别操作）
* 1 关闭防火墙和selinux    
    chkconfig iptables off    
    vim /etc/selinux/config    
    ![](http://ov7z79pcc.bkt.clouddn.com/15037291026190.jpg)
    
* 2 修改hostname，并将域名和ip的映射关系写入hosts文件    
    vim /etc/sysconfig/network（其他4台改为自己的主机名）    
    ![](http://ov7z79pcc.bkt.clouddn.com/15037291270890.jpg)<br>
    vim /etc/hosts<br>
    ![](http://ov7z79pcc.bkt.clouddn.com/15043120325447.jpg)

* 3 同步系统时间    
    ![](http://ov7z79pcc.bkt.clouddn.com/15037291814068.jpg)
    
* 4 创建hadoop用户，并修改其登录密码    
    useradd -m hadoop<br>
    passwd hadoop<br>
    ![](http://ov7z79pcc.bkt.clouddn.com/15037292020082.jpg)
    
* 5 配置免密码登录，切换到hadoop用户<br>
    ![](http://ov7z79pcc.bkt.clouddn.com/15037292207645.jpg)

* 6 生成密钥文件，一路回车即可    
    ssh-keygen –t rsa    
    
* 7 为.ssh目录授权    
    ![](http://ov7z79pcc.bkt.clouddn.com/15037292610825.jpg)
    
* 8 将公钥分别传到5台机器中，并检查5台机器上是否存有5个公钥<br>
    ssh-copy-id hadoop@h1    
    ssh-copy-id hadoop@h2    
    ssh-copy-id hadoop@h3    
    ssh-copy-id hadoop@h4    
    ssh-copy-id hadoop@h5    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043124613928.jpg)

# 安装JDK（在5台主机上分别操作）
* 1 创建放置软件的目录，并将软件上传    
    mkdir /home/hadoop/app

* 2 解压软件包，并创建软连接    
    cd /home/hadoop/app<br>
    tar zxvf java.tar.gz<br>
    ln -s jdk1.7.0_80 jdk
    
* 3 在用户的配置文件中,配置jdk环境变量   
    vim ~/.bashrc<br>
    ![](http://ov7z79pcc.bkt.clouddn.com/15037293678779.jpg)<br>
    重新读取配置文件使其生效 
    . ~/.bashrc
    
* 4 查看是否安装成功    
    ![](http://ov7z79pcc.bkt.clouddn.com/15037294033187.jpg)

# 安装Zookeeper（在5台主机上分别操作）
* 1 首先到官方网站下载ZooKeeper安装包，本实验使用3.4.9版本，将其下载并放到app目录下    
    http://mirrors.cnnic.cn/apache/zookeeper/    
    
* 2 在app目录下对zookeeper安装包解压    
    tar zxf zookeeper-3.4.9.tar.gz<br>
    mv zookeeper-3.4.9 zookeeper
    
* 3 创建数据目录    
    mkdir -p /home/hadoop/data/zookeeper/zkdata<br>
    mkdir -p /home/hadoop/data/zookeeper/zkdatalog
    
* 4 在ZooKeeper安装目录的conf目录下，创建一个配置文件zoo.cfg    
    cd /home/hadoop/app/zookeeper/conf<br>
    cp zoo_sample.cfg zoo.cfg<br>
    vim zoo.cfg<br>
    **配置内容**<br>
    tickTime=2000<br>
    initLimit=10<br>
    syncLimit=5<br>
    dataDir=/home/hadoop/data/zookeeper/zkdata<br>
    dataLogDir=/home/hadoop/data/zookeeper/zkdatalog<br>
    clientPort=2181<br>
    server.1=h1:2888:3888<br>
    server.2=h2:2888:3888<br>
    server.3=h3:2888:3888<br>
    server.4=h4:2888:3888<br>
    server.5=h5:2888:3888
    
* 5 在5台机器上写入各自的myid文件    
    h1上操作: echo 1 > /home/hadoop/data/zookeeper/zkdata/myid<br>
    h2上操作: echo 2 > /home/hadoop/data/zookeeper/zkdata/myid<br>
    h3上操作: echo 3 > /home/hadoop/data/zookeeper/zkdata/myid<br>
    h4上操作: echo 4 > /home/hadoop/data/zookeeper/zkdata/myid<br>
    h5上操作: echo 5 > /home/hadoop/data/zookeeper/zkdata/myid
    
* 6 在每台集群上启动ZooKeeper Server    
    /home/hadoop/app/zookeeper/bin/zkServer.sh start<br>
    zookeeper启动之后，输入jps命令查看进程<br>
    ![](http://ov7z79pcc.bkt.clouddn.com/15043395433450.jpg)

* 7 通过status参数查看每个节点的状态    
    /home/hadoop/app/zookeeper/bin/zkServer.sh status    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043396046137.jpg)
    ![](http://ov7z79pcc.bkt.clouddn.com/15043396200458.jpg)
    ![](http://ov7z79pcc.bkt.clouddn.com/15043396350284.jpg)
    ![](http://ov7z79pcc.bkt.clouddn.com/15043396493235.jpg)
    ![](http://ov7z79pcc.bkt.clouddn.com/15043396636599.jpg)

# hadoop集群环境搭建（在5台主机上分别操作）
* 1 将下载好的hadoop-2.6.0.tar.gz安装包，上传至/home/hadoop/app目录下，解压并创建软链接    
    tar zxvf hadoop-2.6.0.tar.gz    
    ln -s hadoop-2.6.0.tar.gz hadoop
    
* 2 删除没用的软件包，并查看app目录下应该为如下状态    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043401307206.jpg)
    
* 3 配置环境变量    
    vim ~/.bashrc    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043430471081.jpg)
    重新读取配置文件使其生效 
    . ~/.bashrc

* 4 进入/home/hadoop/app/hadoop/etc/hadoop/目录下，配置hadoop-env.sh文件，只修改如下地方
    vim hadoop-env.sh    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043403565437.jpg)

* 5 配置core-site.xml    
    先创建临时目录  mkdir /home/hadoop/data/tmp    
    vim core-site.xml
    ![](http://ov7z79pcc.bkt.clouddn.com/15043411024133.jpg)

* 6 配置hdfs-site.xml    
    创建JournalNode集群在对nameNode的目录进行共享时的存储数据的磁盘路径<br>
    mkdir -p /home/hadoop/data/journaldata/jn    
    vim hdfs-site.xml
    
```xml
<configuration>
	<property>
		<name>dfs.replication</name>
		<value>3</value>
		<!-- 数据块副本数为3 -->
    </property>
	<property>
		<name>dfs.permissions</name>
		<value>false</value>
	</property>
	<property>
		<name>dfs.permissions.enabled</name>
		<value>false</value>
		<!-- 权限默认配置为false -->
	</property>
	<property>
		<name>dfs.nameservices</name>
		<value>cluster1</value>
		<!-- 命名空间，它的值与fs.defaultFS的值要对应，namenode高可用之后有两个namenode，cluster1是对外提供的统一入口 -->
	</property>
	<property>
		<name>dfs.ha.namenodes.cluster1</name>
		<value>h1,h2</value>
		<!-- 指定 nameService 是 cluster1 时的nameNode有哪些，这里的值也是逻辑名称，名字随便起，相互不重复即可 -->
	</property>
	<property>
		<name>dfs.namenode.rpc-address.cluster1.h1</name>
		<value>h1:9000</value>
		<!-- h1 rpc地址 -->
	</property>
	<property>
		<name>dfs.namenode.http-address.cluster1.h1</name>
		<value>h1:50070</value>
		<!-- h1 http地址 -->
	</property>
	<property>
		<name>dfs.namenode.rpc-address.cluster1.h2</name>
		<value>h2:9000</value>
		<!-- h2 rpc地址 -->
	</property>
	<property>
		<name>dfs.namenode.http-address.cluster1.h2</name>
		<value>h2:50070</value>
		<!-- h2 http地址 -->
	</property>
	<property>
		<name>dfs.ha.automatic-failover.enabled</name>
		<value>true</value>
		<!-- 启动故障自动恢复 -->
    </property>
	<property>
		<name>dfs.namenode.shared.edits.dir</name>
		<value>qjournal://h1:8485;h2:8485;h3:8485;h4:8485;h5:8485/cluster1</value>
		<!-- 指定journal -->
	</property>
	<property>
		<name>dfs.client.failover.proxy.provider.cluster1</name>
		<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
		<!-- 指定 cluster1 出故障时，哪个实现类负责执行故障切换 -->
    </property>
    <property>
		<name>dfs.journalnode.edits.dir</name>
		<value>/home/hadoop/data/journaldata/jn</value>
		<!-- 指定JournalNode集群在对nameNode的目录进行共享时，自己存储数据的磁盘路径 -->
    </property>
	<property>
		<name>dfs.ha.fencing.methods</name>
		<value>shell(/bin/true)</value>
    </property>
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/home/hadoop/.ssh/id_rsa</value>
    </property>
	<property>
        <name>dfs.ha.fencing.ssh.connect-timeout</name>
        <value>10000</value>
        <!-- 脑裂默认配置 -->
    </property>
    <property>
		<name>dfs.namenode.handler.count</name>
		<value>100</value>
    </property>
</configuration>
```
    
* 7 配置slave    
    vim slaves    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043423327835.jpg)

# hdfs配置完毕后启动顺序
* 1 在5台主机上分别启动Zookeeper进程
    /home/hadoop/app/zookeeper/bin/zkServer.sh start
    
* 2 在5台主机上分别启动journalnode进程    
    /home/hadoop/app/hadoop/sbin/hadoop-daemon.sh start journalnode    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043431648788.jpg)

* 3 在其中一个namenode上(比如h1)执行格式化    
    * namenode格式化
    cd /home/hadoop/app/hadoop    
    bin/hdfs namenode -format    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043433245868.jpg)

    * 格式化高可用    
    bin/hdfs zkfc -formatZK    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043434063441.jpg)

    * 启动namenode    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043435043403.jpg)

* 4 与此同时，需要在备节点（h2）上执行数据同步    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043436661245.jpg)

* 5 h2同步完数据后，紧接着在h1节点上，按下ctrl+c来结束namenode进程，然后关闭所有节点上面的journalnode进程    
    /home/hadoop/app/hadoop/sbin/hadoop-daemon.sh stop journalnode
    
* 6 如果上面操作没有问题，我们可以在h1主机上一键启动hdfs所有相关进程    
    /home/hadoop/app/hadoop/sbin/start-dfs.sh    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043439653301.jpg)
    ![](http://ov7z79pcc.bkt.clouddn.com/15043440227935.jpg)
    ![](http://ov7z79pcc.bkt.clouddn.com/15043440476326.jpg)

* 7 通过web界面http://h1:50070查看namenode启动情况    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043442909768.jpg)
    ![](http://ov7z79pcc.bkt.clouddn.com/15043443057545.jpg)

* 8 在h1主机上面上传文件至hdfs进行测试    
    cd /home/hadoop/app/hadoop
    vim abc.txt    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043444234503.jpg)
    hadoop fs -mkdir /test    
    hadoop fs -put abc.txt /test    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043445526011.jpg)

# YARN安装配置
* 1 配置mapred-site.xml<br>
    cd /home/hadoop/app/hadoop/etc/hadoop<br>
    vim mapred-site.xml    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043458171622.jpg)

* 2 配置yarn-site.xml    
    vim yarn-site.xml    
    
```xml
<configuration>
	<property>
		<name>yarn.resourcemanager.connect.retry-interval.ms</name>
		<value>2000</value>
		<!-- 超时的周期 -->
	</property>
	<property>
		<name>yarn.resourcemanager.ha.enabled</name>
		<value>true</value>
		<!-- 打开高可用 -->
	</property>
	<property>
		<name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
		<value>true</value>
		<!-- 启动故障自动恢复 -->
	</property>
	<property>
		<name>yarn.resourcemanager.ha.automatic-failover.embedded</name>
		<value>true</value>
	</property>
	<property>
		<name>yarn.resourcemanager.cluster-id</name>
		<value>yarn-rm-cluster</value>
		<!-- 给yarn cluster 取个名字yarn-rm-cluster -->
	</property>
	<property>
		<name>yarn.resourcemanager.ha.rm-ids</name>
		<value>rm1,rm2</value>
		<!-- 给ResourceManager 取个名字 rm1,rm2 -->
	</property>
	<property>
		<name>yarn.resourcemanager.hostname.rm1</name>
		<value>h1</value>
		<!-- 配置ResourceManager rm1 hostname -->
	</property>
	<property>
		<name>yarn.resourcemanager.hostname.rm2</name>
		<value>h2</value>
		<!-- 配置ResourceManager rm2 hostname -->
	</property>
	<property>
		<name>yarn.resourcemanager.recovery.enabled</name>
		<value>true</value>
		<!-- 启用resourcemanager 自动恢复 -->
	</property>
	<property>
		<name>yarn.resourcemanager.zk.state-store.address</name>
		<value>h1:2181,h2:2181,h3:2181,h4:2181,h5:2181</value>
		<!-- 配置Zookeeper地址 -->
	</property>
	<property>
		<name>yarn.resourcemanager.zk-address</name>
		<value>h1:2181,h2:2181,h3:2181,h4:2181,h5:2181</value>
		<!-- 配置Zookeeper地址 -->
	</property>
	<property>
		<name>yarn.resourcemanager.address.rm1</name>
		<value>h1:8032</value>
		<!-- rm1端口号 -->
	</property>
	<property>
		<name>yarn.resourcemanager.scheduler.address.rm1</name>
		<value>h1:8034</value>
		<!-- rm1调度器的端口号 -->
	</property>
	<property>
		<name>yarn.resourcemanager.webapp.address.rm1</name>
		<value>h1:8088</value>
		<!-- rm1 webapp端口号 -->
	</property>
	<property>
		<name>yarn.resourcemanager.address.rm2</name>
		<value>h2:8032</value>
		<!-- rm2端口号 -->
	</property>
	<property>
		<name>yarn.resourcemanager.scheduler.address.rm2</name>
		<value>h2:8034</value>
		<!-- rm2调度器的端口号 -->
	</property>
	<property>
		<name>yarn.resourcemanager.webapp.address.rm2</name>
		<value>h2:8088</value>
		<!-- rm2 webapp端口号 -->
	</property>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
		<value>org.apache.hadoop.mapred.ShuffleHandler</value>
		<!-- 执行MapReduce需要配置的shuffle过程 -->
	</property>
</configuration>
```

* 3 在h1节点上启动YARN    
    /home/hadoop/app/hadoop/sbin/start-yarn.sh    

* 4 在h2节点上面执行<br>
    /home/hadoop/app/hadoop/sbin/yarn-daemon.sh start resourcemanager    
    
* 5 检查一下ResourceManager状态    
    /home/hadoop/app/hadoop/bin/yarn rmadmin -getServiceState rm1<br>
    /home/hadoop/app/hadoop/bin/yarn rmadmin -getServiceState rm2    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043520789320.jpg)

* 6 Wordcount示例测试    
    hadoop jar /home/hadoop/app/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /test/abc.txt /test/out/    
    ![](http://ov7z79pcc.bkt.clouddn.com/15043527006299.jpg)
    ![](http://ov7z79pcc.bkt.clouddn.com/15043527489963.jpg)



